{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 256\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "path = \"/home/kwlee/Projects_gflas/Team_BI/Projects/1.Knockout_project/Data/Results/3.Model_test/MHSA_CNN/target1/set4/checkpoints/latest_net.pth\"\n",
    "file = \"/home/kwlee/Projects_gflas/Team_BI/Projects/1.Knockout_project/Data/Finalsets/Data/Cas9_HF1_wang_parsing.pkl\"\n",
    "#path -> model 경로 입력\n",
    "#file -> 테스트 파일 경로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataWrapper:\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.nuc_to_idx = {\"A\": 0, \"C\": 1, \"G\": 2, \"T\": 3}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"X\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.to_list()\n",
    "\n",
    "        res = dict()\n",
    "        for col in self.data.keys():\n",
    "            if col == \"X\":\n",
    "                res[col] = torch.tensor(\n",
    "                    [self.nuc_to_idx[x] for x in self.data[col][idx]], dtype=torch.long\n",
    "                )\n",
    "            else:\n",
    "                res[col] = torch.tensor(self.data[col][idx], dtype=torch.float)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, batch_size):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.seqlen = 33\n",
    "\n",
    "    def data_set(self, file, n_return, ratio=1.0):\n",
    "\n",
    "        data = pickle.load(open(file, \"rb\"))\n",
    "\n",
    "        data_size = len(data[\"X\"])\n",
    "        indice = list(range(data_size))\n",
    "\n",
    "        np.random.shuffle(indice)\n",
    "\n",
    "        minY = min(data[\"Y\"])\n",
    "        maxY = max(data[\"Y\"])\n",
    "        data[\"Y\"] = [(i - minY) / (maxY - minY) for i in data[\"Y\"]]\n",
    "\n",
    "        if n_return == 3:\n",
    "            test_ratio = 0.15\n",
    "            val_ratio = test_ratio\n",
    "        elif n_return == 2:\n",
    "            test_ratio = 0.15\n",
    "            val_ratio = 0\n",
    "        elif n_return == 1:\n",
    "            test_ratio = 0\n",
    "            val_ratio = 0\n",
    "\n",
    "        train_size = int(np.floor(data_size * (1 - (val_ratio + test_ratio))) * ratio)\n",
    "        valid_size = int(np.floor(data_size * val_ratio))\n",
    "        test_size = int(np.floor(data_size * test_ratio))\n",
    "\n",
    "        indices = dict()\n",
    "        sampler = dict()\n",
    "        indices[\"Val\"] = random.sample(indice[:valid_size], valid_size)\n",
    "        indices[\"Test\"] = random.sample(\n",
    "            indice[valid_size : valid_size + test_size], test_size\n",
    "        )\n",
    "        indices[\"Train\"] = random.sample(\n",
    "            indice[valid_size + test_size : valid_size + test_size + train_size],\n",
    "            train_size,\n",
    "        )\n",
    "\n",
    "        train_set = {\n",
    "            \"X\": [data[\"X\"][i] for i in indices[\"Train\"]],\n",
    "            \"Y\": [data[\"Y\"][i] for i in indices[\"Train\"]],\n",
    "        }\n",
    "        test_set = {\n",
    "            \"X\": [data[\"X\"][i] for i in indices[\"Test\"]],\n",
    "            \"Y\": [data[\"Y\"][i] for i in indices[\"Test\"]],\n",
    "        }\n",
    "\n",
    "        if n_return == 3:\n",
    "            valid_set = {\n",
    "                \"X\": [data[\"X\"][i] for i in indices[\"Val\"]],\n",
    "                \"Y\": [data[\"Y\"][i] for i in indices[\"Val\"]],\n",
    "            }\n",
    "            return train_set, valid_set, test_set\n",
    "        elif n_return == 2:\n",
    "            return train_set, test_set\n",
    "        elif n_return == 1:\n",
    "            return train_set\n",
    "\n",
    "    def loader_only(self, data):\n",
    "      \n",
    "            loader = DataLoader(\n",
    "                DataWrapper(data),\n",
    "                batch_size=batch_size,\n",
    "                num_workers=8,\n",
    "                drop_last=True,\n",
    "            )\n",
    "            return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForeverDataIterator:\n",
    "    \"\"\"A data iterator that will never stop producing data\"\"\"\n",
    "    def __init__(self, data_loader: DataLoader, device=None):\n",
    "        self.data_loader = data_loader\n",
    "        self.iter = iter(self.data_loader)\n",
    "        self.device = device\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            data = next(self.iter)\n",
    "            if isinstance(data, dict):\n",
    "                data = [v for k, v in data.items()]  \n",
    "            if self.device is not None:\n",
    "                data = send_to_device(data, self.device)\n",
    "                \n",
    "        except StopIteration:\n",
    "            self.iter = iter(self.data_loader)\n",
    "            data = next(self.iter)\n",
    "            if isinstance(data, dict):\n",
    "                data = [v for k, v in data.items()]  \n",
    "            if self.device is not None:\n",
    "                data = send_to_device(data, self.device)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM = DataManager(batch_size)\n",
    "test_target_iter = ForeverDataIterator(DM.loader_only(DM.data_set(file, n_return = 1, ratio = 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flattening(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flattening, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, 1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Taken from: https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, dim, dropout=0.1, max_len=43):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0.0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MHSA_CNN(nn.Module):\n",
    "    def __init__(self, len: int):\n",
    "        super(MHSA_CNN, self).__init__()\n",
    "\n",
    "        self.seq_len = len\n",
    "        self.embedding_dim = len + 1\n",
    "        self.dropout_rate = 0.4\n",
    "        self.single_head_size = 32\n",
    "        self.multi_head_num = 8\n",
    "        self.multi_head_size = 100  ###\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(\n",
    "            num_embeddings=4, embedding_dim=self.embedding_dim, max_norm=True\n",
    "        )\n",
    "        self.position_encoding = PositionalEncoding(\n",
    "            dim=self.embedding_dim, max_len=self.seq_len, dropout=0.1\n",
    "        )\n",
    "\n",
    "        self.Q = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(\n",
    "                    in_features=self.embedding_dim, out_features=self.single_head_size\n",
    "                )\n",
    "                for i in range(0, self.multi_head_num)\n",
    "            ]\n",
    "        )\n",
    "        self.K = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(\n",
    "                    in_features=self.embedding_dim, out_features=self.single_head_size\n",
    "                )\n",
    "                for i in range(0, self.multi_head_num)\n",
    "            ]\n",
    "        )\n",
    "        self.V = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(\n",
    "                    in_features=self.embedding_dim, out_features=self.single_head_size\n",
    "                )\n",
    "                for i in range(0, self.multi_head_num)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ModuleList([nn.ReLU() for i in range(0, self.multi_head_num)])\n",
    "        self.MultiHeadLinear = nn.Sequential(\n",
    "            nn.LayerNorm(self.single_head_size * self.multi_head_num),\n",
    "            nn.Linear(\n",
    "                in_features=self.single_head_size * self.multi_head_num,\n",
    "                out_features=self.multi_head_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "        )\n",
    "        self.ConvLayer = nn.Sequential(\n",
    "            nn.Conv1d(4, 32, kernel_size=3, padding=\"same\", stride=1, bias=False),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, kernel_size=3, padding=\"same\", stride=1, bias=False),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.flattening = Flattening()\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2)\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.BatchNorm1d(2673),\n",
    "            nn.Linear(in_features=2673, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(in_features=512, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Linear(in_features=32, out_features=1),\n",
    "        )\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=0.0):\n",
    "        # based on: https://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "        d_k = query.size(-1)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        p_attn = F.dropout(p_attn, p=dropout, training=self.training)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        identity = inputs.clone()\n",
    "\n",
    "        inputs_embs = self.embedding_layer(inputs) * math.sqrt(self.embedding_dim)\n",
    "        inputs_sums = self.position_encoding(inputs_embs)\n",
    "        output = inputs_sums\n",
    "        ##########################################################################\n",
    "\n",
    "        pAttn_concat = torch.Tensor([]).to(inputs.device)\n",
    "        attn_concat = torch.Tensor([]).to(inputs.device)\n",
    "        for i in range(0, self.multi_head_num):\n",
    "            query = self.Q[i](output)\n",
    "            key = self.K[i](output)\n",
    "            value = self.V[i](output)\n",
    "            attnOut, p_attn = self.attention(query, key, value, dropout=0.0)\n",
    "            attnOut = self.relu[i](attnOut)\n",
    "            attn_concat = torch.cat((attn_concat, attnOut), dim=2)\n",
    "\n",
    "        attn_out = self.MultiHeadLinear(attn_concat)\n",
    "        attn_out = self.maxpool(attn_out)\n",
    "        attn_out = self.flattening(attn_out)\n",
    "        # attn_out = self.avgpool(attn_out)\n",
    "\n",
    "        conv_out = F.one_hot(identity).to(torch.float)\n",
    "        conv_out = conv_out.transpose(1, 2)\n",
    "        conv_out = self.ConvLayer(conv_out)\n",
    "        conv_out = self.flattening(conv_out)\n",
    "        # conv_out = self.maxpool(conv_out)\n",
    "\n",
    "        # output = self.flattening(output)\n",
    "        output = torch.cat((attn_out, conv_out), dim=1)\n",
    "        # output = output.reshape(output.shape[0], -1)\n",
    "        output = self.predictor(output)\n",
    "\n",
    "        return output.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MHSA_CNN(len = 33).to(device)\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data):\n",
    "  \n",
    "        eval = {\"predicted_value\": list(), \"real_value\": list()}\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(data)):\n",
    "                X, y = next(data)\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                outputs = model(X)\n",
    "                eval[\"predicted_value\"] += outputs.cpu().detach().numpy().tolist()\n",
    "                eval[\"real_value\"] += y.cpu().detach().numpy().tolist()\n",
    "\n",
    "        corrs = spearmanr(eval[\"real_value\"], eval[\"predicted_value\"])[0]\n",
    "        corrp = pearsonr(eval[\"real_value\"], eval[\"predicted_value\"])[0]\n",
    "        return corrs, corrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation.\t0.6675447458572075\n",
      "Pearson Correlation.\t0.6454832626750231\n"
     ]
    }
   ],
   "source": [
    "corrs, corrp = test(test_target_iter)\n",
    "print(f\"Spearman Correlation.\\t{corrs}\")\n",
    "print(f\"Pearson Correlation.\\t{corrp}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "14cef931255bd958b973d0e700c331f2b900e2680fcdbcb1b0aa41386dd0cdf9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
